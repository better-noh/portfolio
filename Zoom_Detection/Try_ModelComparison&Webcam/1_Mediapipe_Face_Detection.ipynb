{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mJU4ruVlN7M"
   },
   "source": [
    "Usage example of MediaPipe Face Detection Solution API in Python (see also http://solutions.mediapipe.dev/face_detection).\n",
    "\n",
    "## mediapipe 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuGHnkzrk5c-",
    "outputId": "5a21244b-4093-42f1-cf86-21415774c340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.9.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (23.3.3)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (22.2.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe) (4.7.0.72)\n",
      "Collecting sounddevice>=0.4.4\n",
      "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from mediapipe) (1.22.4)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.9/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (5.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->mediapipe) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: sounddevice, mediapipe\n",
      "Successfully installed mediapipe-0.9.3.0 sounddevice-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMZWbFaMS2wy"
   },
   "source": [
    "## 이미지 업로드\n",
    "\n",
    "- short range vs. full range\n",
    "> - An integer index 0 or 1. (정수 인덱스 0 또는 1.)\n",
    "> - Use 0 to select a **short-range model** that works best for faces within 2 meters from the camera, (카메라에서 2m 이내의 얼굴에 가장 적합한 단거리 모델을 선택하려면 0을 사용하고)\n",
    "> - and 1 for a **full-range model** best for faces within 5 meters. (5m 이내의 얼굴에 가장 적합한 전체 범위 모델을 선택하려면 1을 사용합니다.)\n",
    "> - For the **full-range option**, a sparse model is used for its improved inference speed. (전체 범위 옵션의 경우 향상된 추론 속도를 위해 희소 모델이 사용됩니다.)\n",
    "\n",
    "- 참고: [Hugging Face](https://huggingface.co/spaces/kristyc/mediapipe-face-detection/blob/main/app.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 135
    },
    "id": "tzXuqyIBlXer",
    "outputId": "935b5ae9-0542-4c05-9c97-33573324fa1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d40b7ad0-4131-46d8-a7fc-11f9b4d9f670\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-d40b7ad0-4131-46d8-a7fc-11f9b4d9f670\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving yes.png to yes.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-94768e5c-59e5-4972-8b67-09a9a93cd8da\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-94768e5c-59e5-4972-8b67-09a9a93cd8da\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving yes.png to yes (1).png\n"
     ]
    }
   ],
   "source": [
    "# Upload images that contain face(s) within 2 meters from the camera.\n",
    "from google.colab import files\n",
    "uploaded_short_range = files.upload()\n",
    "\n",
    "# Upload images that contain face(s) within 5 meters from the camera.\n",
    "from google.colab import files\n",
    "uploaded_full_range = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zty-avM_TBRU"
   },
   "source": [
    "## 이미지 사이즈 조절\n",
    "- 동일한 이미지로 face detection 할 예정.\n",
    "- Mediapipe의 Face tracking solution을 사용하였고, OpenCV를 사용하여 웹캠을 제어합니다.\n",
    "> - 문장 참고: https://dynamikontrol.readthedocs.io/ko/latest/face_tracking_camera.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8rjHk72-lmHX"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "DESIRED_HEIGHT = 480\n",
    "DESIRED_WIDTH = 480\n",
    "def resize_and_show(image):\n",
    "  h, w = image.shape[:2]\n",
    "  if h < w:\n",
    "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
    "  else:\n",
    "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "  cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_XX8Ao8IJDO",
    "outputId": "07ebb548-9e1b-412f-b7ba-7bb65fa02bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes.png\n"
     ]
    }
   ],
   "source": [
    "# Preview the images.\n",
    "short_range_images = {name: cv2.imread(name) for name in uploaded_short_range.keys()}\n",
    "for name, image in short_range_images.items():\n",
    "  print(name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLA8Tm92I0S5"
   },
   "source": [
    "<img src=\"img/mediapipe_short-range.png\" height=400 width=500>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJjrO3DjIMwT",
    "outputId": "32b85ba2-a78a-4658-e0ca-97f17313ba37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes.png\n"
     ]
    }
   ],
   "source": [
    "full_range_images = {name: cv2.imread(name) for name in uploaded_full_range.keys()}\n",
    "for name, image in full_range_images.items():\n",
    "  print(name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT6BCAjPJz6h"
   },
   "source": [
    "<img src=\"img/mediapipe_full_range.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOKyd9fOlb8b"
   },
   "source": [
    "- 모든 MediaPipe Solutions Python API 예제는 mp.solutions 에 있다.\n",
    "- MediaPipe Face Mesh 솔루션의 경우 `mp_face_detection = mp.solutions.face_detection` 으로 이 모듈에 액세스할 수 있다.\n",
    "- 초기화 중에 `min_detection_confidence` 매개변수를 변경할 수 있다. \n",
    "> - 매개변수에 대한 자세한 정보를 얻으려면 help(mp_face_detection.FaceDetection)를 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9N5glp9lpYL",
    "outputId": "4a8f7ae3-85e3-4333-a45e-0118c6e6697f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FaceDetection in module mediapipe.python.solutions.face_detection:\n",
      "\n",
      "class FaceDetection(mediapipe.python.solution_base.SolutionBase)\n",
      " |  FaceDetection(min_detection_confidence=0.5, model_selection=0)\n",
      " |  \n",
      " |  MediaPipe Face Detection.\n",
      " |  \n",
      " |  MediaPipe Face Detection processes an RGB image and returns a list of the\n",
      " |  detected face location data.\n",
      " |  \n",
      " |  Please refer to\n",
      " |  https://solutions.mediapipe.dev/face_detection#python-solution-api\n",
      " |  for usage examples.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FaceDetection\n",
      " |      mediapipe.python.solution_base.SolutionBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, min_detection_confidence=0.5, model_selection=0)\n",
      " |      Initializes a MediaPipe Face Detection object.\n",
      " |      \n",
      " |      Args:\n",
      " |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for face\n",
      " |          detection to be considered successful. See details in\n",
      " |          https://solutions.mediapipe.dev/face_detection#min_detection_confidence.\n",
      " |        model_selection: 0 or 1. 0 to select a short-range model that works\n",
      " |          best for faces within 2 meters from the camera, and 1 for a full-range\n",
      " |          model best for faces within 5 meters. See details in\n",
      " |          https://solutions.mediapipe.dev/face_detection#model_selection.\n",
      " |  \n",
      " |  process(self, image: numpy.ndarray) -> <function NamedTuple at 0x7fda84705a60>\n",
      " |      Processes an RGB image and returns a list of the detected face location data.\n",
      " |      \n",
      " |      Args:\n",
      " |        image: An RGB image represented as a numpy ndarray.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If the underlying graph throws any error.\n",
      " |        ValueError: If the input image is not three channel RGB.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A NamedTuple object with a \"detections\" field that contains a list of the\n",
      " |        detected face location data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      A \"with\" statement support.\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  close(self) -> None\n",
      " |      Closes all the input sources and the graph.\n",
      " |  \n",
      " |  create_graph_options(self, options_message: google.protobuf.message.Message, values: Mapping[str, Any]) -> google.protobuf.message.Message\n",
      " |      Sets protobuf field values.\n",
      " |      \n",
      " |      Args:\n",
      " |        options_message: the options protobuf message.\n",
      " |        values: field value pairs, where each field may be a \".\" separated path.\n",
      " |      \n",
      " |      Returns:\n",
      " |        the options protobuf message.\n",
      " |  \n",
      " |  reset(self) -> None\n",
      " |      Resets the graph for another run.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "help(mp_face_detection.FaceDetection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGDrWOTncuR4"
   },
   "source": [
    "- MediaPipe 의 얼굴 인식은 엄청 빠른 얼굴 인식 솔루션입니다. \n",
    "> - 6개의 얼굴 랜드마크(오른쪽 눈, 왼쪽 눈, 코끝, 입 중심, 오른쪽 귀 윗가장자리 위의 점 및 왼쪽 귀 윗가장자리 위의 점)와 다중 얼굴 인식 기능을 지원합니다.\n",
    "> - 이 모듈은 가볍고 성능이 뛰어난 얼굴 검출기인 BlazeFace 에 기반을 두었습니다.\n",
    "> - 인식기의 초실시간 기능은 형상 추론(예: 얼굴 그물망)기능 같은 3D 얼굴 키포인트 추출, 얼굴의 특징 또는 표정 판별, 얼굴 부위 분할 같은 작업에 사용할 수 있습니다.\n",
    "> - GPU 없이 CPU 만으로도 작업이 가능합니다. \n",
    "- model_selection (모델 선택)\n",
    "> - 모델 인덱스는 0 또는 1\n",
    "> - 0 을 사용하면 카메라 2m 이내의 부분적 모델 촬영에 적합하고,\n",
    "> - 1은 5m 이내에서 전신 모델을 촬영하는 데 적합합니다.\n",
    "> - 지정하지 않을 경우의 기본값은 0 입니다.\n",
    "- min_detection_confidence (최소 감지 신뢰값)\n",
    "> - 검출에 성공한 것으로 간주할 얼굴의 검출 모델의 신뢰값은 ([0.0, 1.0]) 입니다.\n",
    "> - 기본값은 0.5 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uavHckqNmPzE"
   },
   "outputs": [],
   "source": [
    "# Prepare DrawingSpec for drawing the face landmarks later.\n",
    "# 점으로 6개의 얼굴 랜드마크 표시\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yl_Oiye4mUuo",
    "outputId": "e7ef2858-b748-4550-bd82-b29c0512b7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detections of yes.png:\n"
     ]
    }
   ],
   "source": [
    "# Run MediaPipe Face Detection with short range model.\n",
    "# min_detection_confidence(최소 감지 신뢰값): 기본값(0.5) => 0.2 => 0.8 => 1.2 로 조정.\n",
    "with mp_face_detection.FaceDetection(\n",
    "    min_detection_confidence=1.2, model_selection=0) as face_detection:\n",
    "  for name, image in short_range_images.items():\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Draw face detections of each face.\n",
    "    print(f'Face detections of {name}:')\n",
    "    if not results.detections:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for detection in results.detections:\n",
    "      mp_drawing.draw_detection(annotated_image, detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4x5VY5F0LcIQ"
   },
   "source": [
    "<img src=\"img/mediapipe_short_range-6-randmark.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WW9beRr8lWLq",
    "outputId": "53b29db6-36d6-42b9-f18b-6e6522e0bc1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detections of yes.png:\n"
     ]
    }
   ],
   "source": [
    "# Run MediaPipe Face Detection with full range model.\n",
    "# min_detection_confidence(최소 감지 신뢰값): 기본값(0.5) => 0.2 => 0.8 => 1.2로 조정.\n",
    "with mp_face_detection.FaceDetection(\n",
    "    min_detection_confidence=0.8, model_selection=1) as face_detection:\n",
    "  for name, image in full_range_images.items():\n",
    "    # Convert the BGR image to RGB and process it with MediaPipe Face Detection.\n",
    "    results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Draw face detections of each face.\n",
    "    print(f'Face detections of {name}:')\n",
    "    if not results.detections:\n",
    "      continue\n",
    "    annotated_image = image.copy()\n",
    "    for detection in results.detections:\n",
    "      mp_drawing.draw_detection(annotated_image, detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eTdjBTvMLSQ"
   },
   "source": [
    "<img src=\"img/mediapipe_full_range-6-randmark.png\" height=400 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hF05bC9AVYvn"
   },
   "source": [
    "## test 결과\n",
    "- 마스크 쓴 사람의 경우 랜드마크를 감지하기 어렵기 때문에 얼굴을 잡기 힘들다는 것을 확인.\n",
    "- model_selection (모델 선택, 0 또는 1)에 따라서 얼굴(6개의 랜드마크)을 탐지하는 결과에 차이가 나타타남.\n",
    "- min_detection_confidence(얼굴 감지 모델의 최소 신뢰도 값, [0.0, 1.0])을 0.2, 0.5(기본값), 0.8, 1.2 등으로 조정했으나, 얼굴 탐지 결과가 동일하게 나옴."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
